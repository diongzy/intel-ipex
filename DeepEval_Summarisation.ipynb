{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a93e87-d0e9-4a94-be44-54f7ec2134aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f10db4-06ec-40ab-983b-e8519a5ee603",
   "metadata": {},
   "source": [
    "# DeepEval Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42993e3-6e56-4ad9-932f-a85255203b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SummarizationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = '31143cd4889f4ef8bb2742d3570c2b24'\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://qmed.openai.azure.com/\"\n",
    "\n",
    "def round_up(n, decimals = 0):\n",
    "    multiplier = 10**decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "custom_model = AzureChatOpenAI(\n",
    "    openai_api_version= \"2024-02-15-preview\",\n",
    "    azure_deployment= \"gpt-4-turbo\",\n",
    "    temperature=0,\n",
    "    model_version=\"turbo-2024-04-09\",\n",
    ")\n",
    "\n",
    "\n",
    "azure_openai = AzureOpenAI(model=custom_model)\n",
    "\n",
    "# base_path = '/home/qmed-intel/Desktop/Notebook/Shamus/ipex+tcm+bf16'\n",
    "# file_name = 'inference_ipex+tcm+bf16.csv'\n",
    "# inference_results_path = os.path.join(base_path, file_name)\n",
    "# df = pd.read_csv(inference_results_path)\n",
    "\n",
    "base_path = '/home/qmed-intel/Desktop/Notebook/ZiYu/'\n",
    "file_name = 'test.xlsx'\n",
    "inference_results_path = os.path.join(base_path, file_name)\n",
    "df = pd.read_excel(inference_results_path)\n",
    "\n",
    "score_breakdown = []\n",
    "results = []\n",
    "reason = []\n",
    "start_time = 0\n",
    "end_time = 0\n",
    "exe_time = 0\n",
    "total_exe_time = 0\n",
    "avg_exe_time = 0\n",
    "query = 0\n",
    "score = 0\n",
    "alignment_score = 0\n",
    "coverage_score = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    query += 1\n",
    "    print(\"Query:\", query, \"\\n\")\n",
    "    # print(row['Input_Query'] + \"\\n\")\n",
    "    # print(row['Output_Response'])\n",
    "    # This is the original text to be summarized\n",
    "    input_query = row['Input Query']\n",
    "    \n",
    "    # This is the summary, replace this with the actual output from your LLM application\n",
    "    actual_output = row['Generated Output']\n",
    "    \n",
    "    test_case = LLMTestCase(input=input_query, actual_output=actual_output)\n",
    "    metric = SummarizationMetric(\n",
    "        threshold=0.5, \n",
    "        model=azure_openai, \n",
    "        assessment_questions=[\n",
    "     \"Does this summary clearly state the age and gender of the patient?\", \n",
    "    \"Does it cover the main symptom that concerns the patient the most (Chief Complaint) and the duration it has been present?\", \n",
    "    \"Does it cover the elaboration of the main symptom, additional symptoms associated with the chief complaint and important negative history?\", \n",
    "    \"Does the summary include relevant past medical history, such as chronic conditions, allergy history or previous significant illnesses and information on any past surgical procedures the patient has undergone?\", \n",
    "    \"Does the summary include relevant family medical history and social history on smoking and alcohol consumption?\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    metric.measure(test_case)\n",
    "    end_time = time.time()\n",
    "\n",
    "    exe_time = round_up((end_time - start_time), 4)\n",
    "\n",
    "    total_exe_time += exe_time\n",
    "\n",
    "    print(\"Execution time:\", exe_time, \"\\n\")\n",
    "    \n",
    "    print(metric.score)\n",
    "    print(metric.reason)\n",
    "    print(metric.score_breakdown)\n",
    "    print(\"\\n\")\n",
    "    score_breakdown = metric.score_breakdown\n",
    "    # if metric.score >= 0.85:\n",
    "    results.append({\n",
    "        'Input Query': input_query, \n",
    "        'Generated Summary': actual_output, \n",
    "        'Score': metric.score, \n",
    "        'Reason': metric.reason, \n",
    "        'Alignment Score': score_breakdown['Alignment'], \n",
    "        'Coverage Score': score_breakdown['Coverage'], \n",
    "        'Execution Time (s)': exe_time, \n",
    "    })\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "avg_exe_time = total_exe_time / query\n",
    "\n",
    "print(\"Overall Average Execution Time:\", avg_exe_time)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# Save the DataFrame to a CSV file\n",
    "base_path = '/home/qmed-intel/Desktop/Notebook/Shamus/'\n",
    "file_name = 'deepeval_n=50.csv'\n",
    "deepeval_path = os.path.join(base_path, file_name)\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "\n",
    "df.to_csv(deepeval_path, index=False)\n",
    "print(f\"Results successfully saved to {deepeval_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33959ad1-f87c-4493-aaf1-f0fc368c0d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /home/qmed-intel/Desktop/Notebook/Shamus/ipex+tcm+AMX+bf16/deepeval_n=14.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Correct file path\n",
    "base_path = '/home/qmed-intel/Desktop/Notebook/Shamus/ipex+tcm+AMX+bf16'\n",
    "file_name = 'deepeval_n=14.csv'\n",
    "file_path = os.path.join(base_path, file_name)\n",
    "\n",
    "# Check if the file exists at the new path\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file exists at: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d74a1b1e-feba-43ea-a075-dff60e02c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results successfully saved to HEREEEdeepeval_llama_no_FT_pipeline_50_samples2024-05-28_12-54-33.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "df = pd.DataFrame(results)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = filename = f\"deepeval_llama_no_FT_pipeline_50_samples{current_datetime}.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(f\"Results successfully saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e03fb15f-e96b-439b-9d09-26c74a75985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "good_results\n",
    "df_good_results = pd.DataFrame(good_results)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"deepeval_llama_instruct_raw_50_samples_extracted_less_than_0.8_{current_datetime}.csv\"\n",
    "\n",
    "df_good_results.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86380879-9e58-4b2b-9d1c-f774b044e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row['Score']: 0.75\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.4\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.7647058823529411\n",
      "row['Score']: 0.75\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.4\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.75\n",
      "row['Score']: 0.6\n",
      "row['Score']: 0.6\n"
     ]
    }
   ],
   "source": [
    "bad_results = []\n",
    "existing_csv_file = \"deepeval_colon_rows_2024-05-14_12-32-06.csv\"\n",
    "df = pd.read_csv(existing_csv_file)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Score'] < 0.8:\n",
    "        print(\"row['Score']:\", row['Score'])\n",
    "        bad_results.append({\n",
    "            'Input Query': row['Input Query'], \n",
    "            'Output Response': row['Output Response'], \n",
    "            'Score': row['Score'], \n",
    "            'Reason': row['Reason'], \n",
    "            'Alignment Score': row['Alignment Score'], \n",
    "            'Coverage Score': row['Coverage Score'], \n",
    "            'Execution Time (s)': row['Execution Time (s)'], \n",
    "        })\n",
    "\n",
    "df_bad_results = pd.DataFrame(bad_results)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"deepeval_colon_rows_lower_than_80_{current_datetime}.csv\"\n",
    "\n",
    "df_bad_results.to_csv(filename, index=False)\n",
    "\n",
    "# df_bad_results = pd.DataFrame(bad_results)\n",
    "# current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# filename = f\"deepeval_gpt_bad_results2_{current_datetime}.csv\"\n",
    "\n",
    "# df_bad_results.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a71c5fe3-a406-4293-bf63-d2eb544ac783",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"deepeval_mistral_instruct_qlora_{current_datetime}.csv\"\n",
    "\n",
    "df_results.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Env (with Poetry)",
   "language": "python",
   "name": "my-env-with-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
